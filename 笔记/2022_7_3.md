# 一、神经网络基础
#### 定义神经网络类的方法
```
#所有神经网络应为Moudle的子类
#例子
class MLP（nn.Moudle）:
    def __init__(self):
        super().__init__()  #继承Moudle的内部参数
        #定义特有的内部参数
        self.hidden = nn.Linear(20, 256)
        self.out  = nn.Linear(256, 10)
       
    def forward(self, X):   #构造函数中已调用
        return self.out(F.relu(self.hidden(X)))
       
 #使用神经网络类
 net=MLP()
 net(X)
 nn.Sequential()
```
#### 访问神经网络类的参数
```
#例子
net=nn.Sequential(nn.Linear(4,8), nn>relu(), nn.Linear(8, 1))
net[2].state_dict()  #输出nn.Linear(8,1)的weights和bias
net[2].bias     #输出nn.Linear(8,1)的bias，会被标注rqeuires_grad
net[2].bias.data    #输出输出nn.Linear(8,1)的bias的数据
net[2].weight.grad      #访问weight的梯度
#一次性访问所有参数
*[(name, param.shape) for name, param in net[0].named_parameters()]
#net[0]的所有参数
*[(name, param.shape) for name, param in net.named_parameters()]
#net的所有参数
net.state_dict()['2.bias'].data #根据变量名访问参数
```
```
#!!!比较复杂
#嵌套快获得参数
#多层的块
def block1():
    retrun nn.Sequential(nn.Linear(4, 8), nn.Relu(), nn/Linear(8, 4), nn.Relu())
def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_moudle(f'block(i)', block1())
    return net
rgnet = nn.Sequential(block2(), nn.Linear(4,1))
 print(rgent)
```
模型结构
![861ab98088beaaf1a24fcbf481cc66cf.png](en-resource://database/614:1)

#### 初始化参数

```
def init_normal(m):
    if type(m) == nn.Linear:    #对线性模型初始化，为什么不对非线性初始化？不必要吗？（因为还没学到）
        nn.init.normal_(m.weight, mean=0, std=0.0)      #正态分布初始化weight
        #nn.init.constant_(m.weight, 1)#使用常数初始化权重
        nn.init.zeros_(m.bias)
net.apply(init_normal)          #对net中所有层使用init_normal函数

```
#### 共享权重
```
shared = nn.Linear(8,8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU, nn.Linear(8, 1))
#这个网络中net[2]和net[4]就实现了共享参数
```

#### 读写文件
```
torch.save(x, 'file_name')      #在当前文件夹存储文件file_name
x2 = torch.load(x, 'file_name')      #在当前文件夹加载文件file_name

torch.save(net.state_dict(), ''params)  #将模型参数存进params里
clone=net()     #同样结构的模型
clone.load_state_dict(torch.load(params))       #加载模型参数

```
# GPU的使用（linux和colab）
```
!nvidia-smi     #查看gpu情况

import torch
from torch import nn
torch.device('cpu')     #默认使用cpu
torch.cuda.device('cuda')       #默认使用cuda/GPU:0号
torch.cuda.device('cuda:1')     #默认使用GPU：1号

torch.cuda.device_count()

#以下函数实现无GPU时运行代码
def try_gpu(i=0):
    if torch.cude.device_count()>=i+1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
def try_all_gpus():
    devices = [ torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
    
#使用
#数据在同一个gpu上可以进行计算
x=torch.ones(2,3, device=try_gpu())
net = net.to(device=try_gpu())
#确认模型参数存储在一个gpu上
net[0].weight,data,device

```
# 卷积
 ## 原理
 #### 全连接到卷积的两个原则

1.  平移不变性：识别器对像素特征的识别不会因为位置而改变
2.  局部性：寻找目标只需局部信息，不需要全局信息

#### 卷积层
![cc083d838e3c9a9a1e5f353a7e290b91.png](en-resource://database/616:2)
**卷积核大小是超参数**，控制局部性

```
#卷积底层代码
def corr2d(X, K):   #交叉相关运算
    h, w = K.shape
    Y = torch.zeros(X.shape[0]-h+1. X.shape[1]-w+1)
    for i in range(Y.shape[0])
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i+h], j:j+w] *K).sum()
    return Y

#卷积层
class Conv2D(nn.Module):
    def __init__(self, kernel_size):
        super().__init__()
        self.weight = nn.Parameter(torch.rand(kernel_size))
       self.bias = nn.Parameter(torch.zeros(1))
    def forward(self, x):
        return corr2d(x, sele.weight)+self,bias
```

### 卷积填充与步辐（超参数）
#### 填充
在周围添加额外的行/列，以至于卷积后的特征不会太小
![cc083d838e3c9a9a1e5f353a7e290b91.png](en-resource://database/616:2)
![35260587e7cc7d8a41113fc1809450cf.png](en-resource://database/618:1)
![886db37f409fcc244c01b34758ca2855.png](en-resource://database/620:1)
**保证形状不变**
#### 步幅
指卷积核行列滑动的步长
对于较大的图片/特征，设置大的步幅可以减少计算量/参数量
给定了，卷积核大小k，特征大小n，填充长度p，对于步幅s
![27d789303e1f8b9ae48160751593a1df.png](en-resource://database/622:1)

```
import torch
from torch import nn

def comp_conv2d(conv2d, X):
    X = X.reshape((1, 1)+ X.shape)  #（1，1）加入一个通道和batch的维度
    Y = conv2d(X)
    return reshape(Y.shape[2:])
conc2d = nn.Conv2d(1,1, kernel_size=3, padding=1)   #8=8-3+1*2（pad=1）+1
#padding表示上下各填充一行，所以变成填充两行？输出=输入-卷积核+2*pad+1
X = torch.rand(size=(8, 8))
comp+conv2d(conv2d, X).shape

```
填充：输出=输入-卷积核+2* pad+1
步幅：输出=（输入-卷积核+2* pad+步幅）/步幅

### 通道（输出通道：超参数）

* 每个输入通道有独立的二维卷积核，所有通道的结果相加得到一个输出通道结果

* 每个输出通道有都立的三维卷积核，结果为4维（batch* 通道* 矩阵）

![f4008c761f51f70ec0fa997d0eb54574.png](en-resource://database/624:1)
![2ceeaeab28028da673dea17f6ef650ee.png](en-resource://database/626:1)

* 每个输出通道可识别不同的特征/模式

* 输入通道，识别并且组合特征/模式

#### 1* 1卷积层
**1* 1卷积用于通道融合**
如下，将3通道卷积变为2通道卷积
![ce5463b8aa4cde002e18865658a109a1.png](en-resource://database/628:1)
类似全连接层？
#### 二维卷积
![9fe357db1d99b02c9569c822d26c8c33.png](en-resource://database/630:1)


   ```
   #多输入的卷积运算
   #使用zip将对应通道的特征和卷积核打包，逐一卷积运算再将结果相加
def corr2d_multi_in(X, K):
    return sum(d2l.corr2d(x,k) for x, k in zip(X, K))

#多通道的输出
#对隐藏层的特征，使用多个卷积核，将结果堆叠，就得到多通道的输出
def corr2d_multi_out(X, K):
    return torch.stack([corr2d_multi_in(x,k) for k in K], 0)
K=torch.stack((K, K+1, K+2), 0)
```
### 池化
* 作用：缓解卷积对位置的敏感
* 输入通道=输出通道
* 可填充，可调步幅
#### （二维）最大池化
返回滑动窗口中的最大值
![4a263d9d1dbd88d263505d5729cdde57.png](en-resource://database/632:1)
#### 平均池化
返回滑动窗口中元素的平均值

## LeNet

* 使用数据集MNIST

输入（32x32）-->卷积(6x28x28)-->最大池化（6x14x14）-->卷积(16x10x10)-->最大池化(16x5x5)-->全连接（120）-->全连接（64）-->全连接(10)-->softmax()->>输出（10）

```

net2 = torch.nn.Sequential(
    #Reshape(),
    nn.Conv2d(1, 6, kernel_size=5, padding=2),  #28-5+4+1=28
    nn.ReLU(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*5*5, 120),
    nn.ReLU(),
    nn.Linear(120, 84),
    nn.ReLU(),
    nn.Linear(84, 10)
)


```

## AlexNet

* 使用数据集：ImageNet
* 主要改进：
    1、使用dropout
    2、激活函数使用ReLu,以减缓梯度消失
    3、使用MaxPooling替换AvgPooling

输入(3x224x224)-->
卷积(k=11,p=1,s=4,a=relu)(96x54x54)-->
最大池化(size=3,s=2)(96x26x26)-->
卷积(k=5,p=2,a=relu)(256x26x26)-->
最大池化(size=3,s=2)(256x12x12)-->
卷积(k=3,p=1,a=relu)(384x12x12)-->
卷积(k=3,p=1,a=relu)(384x12x12)-->
卷积(k=3,p=1,a=relu)(256x12x12)-->
最大池化(size=3,s=2)(256x5x5)-->
dense(a=relu)(1x4096)-->
dropout(0.5)-->
dense(a=relu)(1x4096)-->
dropout(0.5)-->
dense(1x10)
```
#AlexNet

net = nn.Sequential(
    nn.Conv2d(1, 96, kernel_size=11, stride=4,padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Conv2d(96, 256, kernel_size=5, padding=2),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Conv2d(256, 384, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(384, 384, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.Conv2d(384, 256, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(kernel_size=3, stride=2),
    nn.Flatten(),       #？？？
    nn.Linear(6400,4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096,4096),
    nn.ReLU(),
    nn.Dropout(p=0.5),
    nn.Linear(4096, 10)
)

#训练代码，使用d2l

lr=0.01
num_epochs = 10
batch_size=128
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)
d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())

```

## VGG

* 思想：相似块的重复使用，有很多变种
* 更大更深的AlexNet
* 参考AlexNet的结构构建VGG块，多个VGG块链接后接全连接层
* 全连接层过大，容易过拟合

VGG块：（卷积数，卷积核大小，输出通道数）
for x in 卷积数：
    卷积(输入通道，输出通道，k=3，p=1)
    输入=输出
    ReLU()

VGG
输入(1,3,224,224)-->
vgg块1输出(n=1)(b,64,112,112)-->
vgg块2输出(n=1)(b,128,56,56)-->
vgg块3输出(n=2)(b,256,28,28)-->
vgg块4输出(n=2)(b,512,14,14)-->
vgg块5输出(n=2)(b,512,7,7)-->
全连接0输出(b，4096)-->
nn.ReLU()-->dropout(0.5)-->
全连接1输出(b,4096)-->
nn.ReLU()-->dropout(0.5)-->
全连接2输出(b,10)
```

def vgg_block(num_convs, in_channels, out_channels):
    layers=[]
    for _ in range(num_convs):
        #VGG块中卷积核大小固定为3，填充为1
        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        layers.append(nn.ReLU())
        #第一个块后，所有的VGG块的输入输出通道数都为out_channels
        in_channels=out_channels
    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
    return nn.Sequential(*layers)
#VGG块的设定(卷积核数，输出通道数)
conv_arch = ((1,64), (1, 128), (2,256), (2,512), (2, 512))
#VGG11
def vgg(conv_arch):
    conv_blks=[]
    in_channels = 1
    #卷积
    for (num_convs, out_channels) in conv_arch:
        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
        #下个输入通道数等于上个块的输出数
        in_channels=out_channels
    return nn.Sequential(
        *conv_blks,
        nn.Flatten(),
        nn.Linear(out_channels*7*7, 4096),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(4096,4096),
        nn.ReLU(),
        nn.Dropout(0.5),
        nn.Linear(4096, 10)
    )
net = vgg(conv_arch)


```

## NiN(网络中的网络)
### NiN块

1. **结构**：一个卷积层后跟两个全连接层(实际上是输入输出大小一致的1x1卷积，步幅1，起全连接层的作用)
2. 交替使用NiN块核步幅为2的最大池化，以逐步减小高宽并增大通道数
3. 最后使用全局平均池化得到输出（输入通道是类别数），代替全连接
4. 不容易拟合，参数更少
5. ![d6bc73263fe8e53d96ae480bfab23abb.png](en-resource://database/636:1)
(batch_size, 输出通道数，高，宽)
输入（b，1，224，224）
NiN块(b，96，54，54)-->
MaxPool(k=3，s=2)(b，96，26，26)-->
NiN块(b，256，26，26)-->
MaxPool(k=3，s=2)(b，256，12，12)-->
NiN块(b，384，12，12)-->
MaxPool(k=3，s=2)(b，384，5，5)-->
NiN块(b，10，5，5)-->
Global AvgPoo(b，10，1，1)-->
nn.Flatten(b,10)

```

import torch
from torch import nn, relu
from d2l import torch as d2l
def nin_block(in_channels, out_channels, kernel_size, strides, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),
        nn.ReLU(),
        nn.Conv2d(out_channels, out_channels, kernel_size=1),
        nn.ReLU()
    )
net = nn.Sequential(
    nin_block(1,96,kernel_size=11,strides=4,padding=0),
    nn.MaxPool2d(3, stride=2),
    nin_block(96,256,kernel_size=5,strides=1,padding=2),
    nn.MaxPool2d(3,stride=2),
    nin_block(256,384,kernel_size=3,strides=1,padding=1),
    nn.MaxPool2d(3, stride=2),
    nn.Dropout(0.5),
    nin_block(384,10,kernel_size=3,strides=1,padding=1),
    nn.AdaptiveAvgPool2d((1,1)),    #结构高宽变为1
    nn.Flatten()
)

```

## GoogleLeNet/Inception V3
### Inception块
GoogleLeNet由Inception块组成：

**Inception结构图**
![5beb59c12b27d902153622e8406d3469.png](en-resource://database/638:1)
四条并行路径

   1. 从不同空间获取信息
   2. 都使用1x1卷积改变通道数
   3. 四条路径通道数应保持一致


```
class Inception(nn.Module):
    #c1-c4是四条路径的通道数
    def __init__(self, in_channels, c1, c2, c3, c4, **kwargs):
        super(Inception, self).__init__(**kwargs)
        #c1,一个1x1卷积
        self.p1_1=nn.Conv2d(in_channels, c1, kernel_size=1)
        #c2, 1x1卷积+3x3卷积
        self.p2_1=nn.Conv2d(in_channels, c2[0], kernel_size=1)
        self.p2_2=nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        #c3, 1x1卷积+5x5卷积
        self.p3_1=nn.Conv2d(in_channels, c3[0], kernel_size=1),
        self.p3_2=nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        #c4,3x3maxpool+1x1卷积
        self.p4_1=nn.MaxPool2d(kernel_size=3, stride=1,padding=1)
        self.p4_2=nn.Conv2d(c4[0], c4[1], kernel_size=1)
   
    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(F.relu(self.p4_1(x))))
        #在通道维度上连接
        return torch.cat((p1,p2,p3,p4), dim=1)

```

### GoogleLeNet
```

b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size==7,stride=2, padding=3),
                    nn.ReLU(),
                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                    nn.Conv2d(64,64,kernel_size=1),
                    nn.ReLU(),
                    nn.Conv2d(64,192,kernel_size=3,padding=1),
                    nn.ReLU(),
                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
)#bx192x12x12
b2 = nn.Sequential(Inception(192, 64, (96,128), (16,32), 32),
                    #64+128+32+32=256
                    Inception(256, 128, (128,192), (32,96), 64),
                    nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
)#bx480x6x6
                    #128+192+96+64
b3 = nn.Sequential(Inception(480,192,(96,208),(16,48),64),
                    Inception(512,160,(112,224),(24,62),64),
                    Inception(512,128,(128,256),(24,64),64),
                    Inception(512,112,(144,288),(32,64),64),
                    Inception(528,256,(160,320),(32,128),128),
                    nn.MaxPool2d(kernel_size=3,stride=2,padding=1)
)#bx832x3x3
                #256+320+128+128=832
b4=nn.Sequential(Inception(832,256,(160,320),(32,128),128),
                    Inception(832,384,(192,384),(48,128),128),
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten()
)#bx1024
net = nn.Sequential(b1,b2,b3,b4,nn.Linear(1024,10))#bx10

```

### 批量归一化
#### 问题：
1. 损失在模型最后，后面的层训练块；
2. 数据在模型最前端，最前端训练较慢，前端变化会导致之后的层也需要变化-->收敛变慢

#### 思想

1. 若每一层都符合某一分布，该模型会比较稳定-->固定小批量里的均值和方差
2. B：批量， x：输入；![bae45578a59eb0d27b73c10a445b1258.png](en-resource://database/640:1)：方差， ![a5c38848aa4cf497405862648a15c0a1.png](en-resource://database/642:1)：均值

![fc72cc3cb9b9c0bb188bf9c49a8af7b6.png](en-resource://database/644:1)
**可学习参数：![9e6e6fe9eea1e80df139f4a73095292b.png](en-resource://database/652:1)和![853222095d10d33d19764addea4621d2.png](en-resource://database/650:1)**
![ae2b63aac3cbad137d2ab573d73837a3.png](en-resource://database/646:1)

3. 作用在：
    1. 全连接和卷积输出上，激活函数之前
    2. 全连接和卷积的输入上
    3. 在全连接层，作用在特征维度
    4. 在卷积层，作用在通道维度
4. 本质：有人指出有可能是在每个批量里加入噪音来控制模型复杂度
![3e7d7d06cda75af3165876539a19afa9.png](en-resource://database/654:1)

5. 可以不用和dropout一起混合使用
6. 可以加速收敛，一般不会改变模型精度
```
#mvoing_mean 方差；moving_var均值
def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):
    #判断是test还是train
    if not torch.is_grad_enabled():
        #test
        X_hat = (X-moving_mean)/torch.sqrt(moving_var+eps)
    else:
        assert len(X.shape) in (2,4)
        #判断输入维度是2D（全连接）还是4D（卷积）
        if len(X.shape)==2:
            mean = X.mean(dim=0)    #对特征进行归一化
            var = ((X-mean)**2).mean(dim=0)
        else:
            #卷积要在通道维上进行计算axis=1
            mean = X.mean(dim=(0,2,3), keepdim=True)
            var = ((X-mean)**2).mean(dim=(0,2,3), keepdim=True)
        X_hat = (X-mean)/torch.sqrt(var+eps)
        moving_mean=momentum*moving_mean+(1.0-momentum)*mean
        moving_var = momentum*moving_var+(1.0-momentum)*var
    Y=gamma*X_hat +beta
    return Y, moving_mean.data, moving_var.data
class BatchNorm(nn.Module):
    # num_features：完全连接层的输出数量或卷积层的输出通道数。
    # num_dims：2表示完全连接层，4表示卷积层
    def __init__(self, num_features, num_dims):
        super().__init__()
        if num_dims == 2:
            shape=(1, num_features)
        else:
            shape = (1, num_features,1,1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 非模型参数的变量初始化为0和1
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.ones(shape)
    def forward(self, X):
        #保证数据在同一个device，方便计算
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var
        Y, self.moving_mean, self.moving_var = batch_norm(X, self.gamma, self.beta, self.moving_mean, self.moving_var, eps=1e-5, momentum=0.9)
        return Y
#在构建模型时使用BatchNorm,LeMet
net = nn.Sequential(
    nn.Conv2d(1, 6, kernel_size=5),
    BatchNorm(6, num_dims=4), nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Conv2d(6, 16, kernel_size=5),
    BatchNorm(16, num_dims=4),nn.Sigmoid(),
    nn.AvgPool2d(kernel_size=2, stride=2),
    nn.Flatten(),
    nn.Linear(16*4*4, 120),
    BatchNorm(120, num_dims=2),nn.Sigmoid(),
    nn.Linear(120, 84),
    BatchNorm(84, num_dims=2),nn.Sigmoid(),
    nn.Linear(84, 10)
)
```
## ResNet

* 作用
    * 残差结构使网络更容易训练
    * 不用担心模型深度过深出现梯度消失的问题(靠残差连接的输入保证)

### 残差块
![498daba6fc4d755d3a84373db440fa4f.png](en-resource://database/656:1)

* 设残差块中的非线性变化为g(x),残差结构的f(x)公式为：f(x)=g(x)+x
* 也可能在右边增加1x1的卷积来改变通道大小（如果非线性变化中改变了通道数）

**种类**
1.  高宽减半ResNet（步幅2）
2.  高宽不变ResNet

```

class Residual(nn.Module):  
    #stride为2时，特征高宽减半
    def __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1):
        super().__init__()
        self.conv1 = nn.Conv2d(input_channels, num_channels, kernel_size=3, padding=1, stride=strides)
        self.conv2 = nn.Conv2d(num_channels, num_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(input_channels, num_channels, kernel_size=1, stride=strides)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(num_channels)
        self.bn2 = nn.BatchNorm2d(num_channels)
    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        Y += X
        return F.relu(Y)
def resnet_block(input_channels, num_channels, num_residuals, first_block=False):
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(input_channels, num_channels, use_1x1conv=True, strides=2))
        else:
            blk.append(Residual(num_channels, num_channels))
    return blk
b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                   nn.BatchNorm2d(64),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
)
b2 = nn.Sequential(*resnet_block(64, 64, 2, first_block=True))
b3 = nn.Sequential(*resnet_block(64, 128, 2))
b4 = nn.Sequential(*resnet_block(128, 256, 2))
b5 = nn.Sequential(*resnet_block(256, 512, 2))
net = nn.Sequential(b1, b2, b3, b4, b5,
                    nn.AdaptiveAvgPool2d((1,1)),
                    nn.Flatten(),
                    nn.Linear(512, 10)
)

```

