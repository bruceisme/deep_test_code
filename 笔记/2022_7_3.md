
# 一、神经网络基础
#### 定义神经网络类的方法
```
#所有神经网络应为Moudle的子类
#例子
class MLP（nn.Moudle）:
    def __init__(self):
        super().__init__()  #继承Moudle的内部参数
        #定义特有的内部参数
        self.hidden = nn.Linear(20, 256)
        self.out  = nn.Linear(256, 10)
       
    def forward(self, X):   #构造函数中已调用
        return self.out(F.relu(self.hidden(X)))
       
 #使用神经网络类
 net=MLP()
 net(X)
 nn.Sequential()
```
#### 访问神经网络类的参数
```
#例子
net=nn.Sequential(nn.Linear(4,8), nn>relu(), nn.Linear(8, 1))
net[2].state_dict()  #输出nn.Linear(8,1)的weights和bias
net[2].bias     #输出nn.Linear(8,1)的bias，会被标注rqeuires_grad
net[2].bias.data    #输出输出nn.Linear(8,1)的bias的数据
net[2].weight.grad      #访问weight的梯度
#一次性访问所有参数
*[(name, param.shape) for name, param in net[0].named_parameters()]
#net[0]的所有参数
*[(name, param.shape) for name, param in net.named_parameters()]
#net的所有参数
net.state_dict()['2.bias'].data #根据变量名访问参数
```
```
#!!!比较复杂
#嵌套快获得参数
#多层的块
def block1():
    retrun nn.Sequential(nn.Linear(4, 8), nn.Relu(), nn/Linear(8, 4), nn.Relu())
def block2():
    net = nn.Sequential()
    for i in range(4):
        net.add_moudle(f'block(i)', block1())
    return net
rgnet = nn.Sequential(block2(), nn.Linear(4,1))
 print(rgent)
```
模型结构
![861ab98088beaaf1a24fcbf481cc66cf.png](en-resource://database/544:1)

#### 初始化参数

```
def init_normal(m):
    if type(m) == nn.Linear:    #对线性模型初始化，为什么不对非线性初始化？不必要吗？（因为还没学到）
        nn.init.normal_(m.weight, mean=0, std=0.0)      #正态分布初始化weight
        #nn.init.constant_(m.weight, 1)#使用常数初始化权重
        nn.init.zeros_(m.bias)
net.apply(init_normal)          #对net中所有层使用init_normal函数

```
#### 共享权重
```
shared = nn.Linear(8,8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU, nn.Linear(8, 1))
#这个网络中net[2]和net[4]就实现了共享参数
```

#### 读写文件
```
torch.save(x, 'file_name')      #在当前文件夹存储文件file_name
x2 = torch.load(x, 'file_name')      #在当前文件夹加载文件file_name

torch.save(net.state_dict(), ''params)  #将模型参数存进params里
clone=net()     #同样结构的模型
clone.load_state_dict(torch.load(params))       #加载模型参数

```
# GPU的使用（linux和colab）
```
!nvidia-smi     #查看gpu情况

import torch
from torch import nn
torch.device('cpu')     #默认使用cpu
torch.cuda.device('cuda')       #默认使用cuda/GPU:0号
torch.cuda.device('cuda:1')     #默认使用GPU：1号

torch.cuda.device_count()

#以下函数实现无GPU时运行代码
def try_gpu(i=0):
    if torch.cude.device_count()>=i+1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')
def try_all_gpus():
    devices = [ torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]
    
#使用
#数据在同一个gpu上可以进行计算
x=torch.ones(2,3, device=try_gpu())
net = net.to(device=try_gpu())
#确认模型参数存储在一个gpu上
net[0].weight,data,device

```
