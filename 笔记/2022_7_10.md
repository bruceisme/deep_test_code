# DenseNet

* 与残差网络的差别在于，稠密连接的结果是拼接起来，而非直接相加，
* 残差连接一个残差结构相加一次，稠密连接结构拼接多次

* 这会导致后期的特征通道非常大，需要过渡层控制通道数
![0bb4cf0193aad05e296f40c72511e72e.png](en-resource://database/680:1)

![e5cd2f0f7d7fb8691614ec1b188cadd3.png](en-resource://database/682:1)


# 卷积模型测试结果
batch=128，epochs=20
## LeNet（sigmod）
loss=0.281， train_acc=0.894, test_acc=0.873
![21e95f5620eace29dde834a5fc435bf9.png](en-resource://database/698:1)
注：使用sigmod时，学习率设置为0.1时，模型收敛减慢明显，20epoch后未完全收敛。loss=0.481, train_acc=0.821, test_acc=0.798
## LeNet(ReLU)

* ReLU对学习率敏感的例子
1. lr=0.9，未能收敛
![e0e484330343af1e83f14e39757245d0.png](en-resource://database/700:1)

2. lr=0.1。loss=0.265， train_acc=0.901, test_acc=0.873
![8b8fb74394f5a2a072e0886bf39d9c65.png](en-resource://database/702:1)
**注：相对于使用sigmod，lr=0.9时，ReLU设置lr=0.1，收敛速度未变慢**

## AlexNet

* batchsize设置过大反而会降低模型的收敛的效率
1. batch_size=256,loss=0.308, train_acc=0.888, test_acc=0.885
![12cdbda3e8163c916620d0b65491eec6.png](en-resource://database/704:1)

2. batch_size=128,loss=0.249, train_acc=0.909, test_acc=0.903
![283508a23bebc14ac36f4aede672f2f0.png](en-resource://database/706:1)

## VGG
batch_size=128,loss=0.044,train_acc=0.984,test_acc=0.921
很慢(batch_size设置为256时Gpu跑不动)
![ecd141bf63ae0c3dcb2120f8727b8288.png](en-resource://database/708:1)


## NiN
比VGG更深，但也明显更快，加速效果明显，收敛更慢，20epoch还没到最优（VGG的计算量主要在全连接层，GPU内存的使用明显减少）
batch_size=128,loss=0.258,train_acc=0.904,test_acc=0.885
![b8ebb1a1cfd6e0cde22b7d937eb67e0e.png](en-resource://database/710:1)



## Inception
loss=0.158,train_acc=0.938,test_acc=0.887
![48b38b5a776635f3c4f4c59446bd0f59.png](en-resource://database/712:1)

## ResNet
前期收敛快，速度比Inception快，学习能力强，第10个epoch左右就学习到了数据所有的特征，但未出现过拟合。
**loss=0,train_acc=100,test_acc=0.929**
![e3825cf58a79d1ee59afd57bd01c29fc.png](en-resource://database/714:1)
## DenseNet

* 比残差要慢，但test集精度差不多，比VGG快

残差和稠密都比其他模型优秀，表明将原始输入加入到模型过程中能使模型得到明显性能上的提升（原因或许是能弥补上计算过程中丢失的特征？）
loss=0.053,train_acc=0.980,test_acc=0.928
![59526a52ca1a1fba0af738d058e097df.png](en-resource://database/716:1)







# 深度学习硬件
![5cc52dfe265ea88cf74d66bf79dc18ec.png](en-resource://database/696:1)
数据并行：将小批量分成n块，每个GPU拿到完整参数技术以块数据的梯度，性能会更好
模型并行：将模型分成n块，每个gpu拿到一块模型技术它的前向和方向结构，通常用于模型达到单GPU放不下
### 多GPU训练
```
%matplotlib inline
import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l

def get_params(params, device):
    new_params=[p.clone().to(device) for p in params]
    for p in new_params:
        p.requires_grad_()
   return new_params
   
new_params = get_params(params, d2l.try_gpu(0))
print(''b1 weight:", new_params[1])
print('b1 grad:' new_params[1].grad)


# allreduce函数将所有向量相加，并将结果广播给所有GPU
def allreduce(data):
    for i in range(1, len(data)):
        data[0][:]+=data[i].to(data[0].device)
    for i in range(1, len(data)):
        data[i]=data[0].to(data[i].devie)

data = [torch.ones((1, 2), device=d2l.try_gpu(i)) * (i + 1) for i in range(2)]
print('allreduce之前：\n', data[0], '\n', data[1])
allreduce(data)
print('allreduce之后：\n', data[0], '\n', data[1])


#将一个小批量数据均匀分布在多个GPU上
data=torch.arange(20).reshape(4,5)
devices = [torch.device('cude:0'), torch.device('cuda1')]
split=nn.parallel.scatter(data, devices)
print('input:' , data)
print('load into:', devices)
#将一个批次数据均匀分布到多个GPU上
def split_batch(X, YU, devices):
    assert X.shape[0] == Y.shape[0]
    return (nn.parallel.scatter(X, devices), nn.parallel.scatter(Y, devices))
    
 #多GPU训练
 def train_batch(X, y, device_params, devices, lr):
    X_shards, y_shards = split_batch(X, y, devices)
    ls = [loss(lenet(X_shard, device_W), y_shards).sum() for X.shards, y_shards, device_W in zip(X_shards, y_shards, device_params)]
    for l in ls:
        l.backward()
    with torch.no_grad():
        for i in range(len(device_params[0])):
            allreduce([device_params[c][i].grad for c in range(len(device)) ])
   for param in device_params:
        d2l.sgd(param, lr, X.shape[0])
```

# 数据增广/强
通过变形数据来获得多样性，是模型泛化性更好，更鲁棒
语音：在背景中加入各种噪音
图像：改变图片的颜色和形状
    1.切割，随机高宽比，随机大小，随机位置
    2.颜色，改变色调，饱和度，明亮度
```
#展示图片
d2l.set_figsize()
img=d2l.Image.open(...)
d2l.plt.imshow()

#对图片进行修改,并输出,aug:tochvision中图像变化方法，
def apply(img, aug, num_rows=2, num_cols=4, scale=1.5):
    Y = [aug(img) for _ in range(num_rows*num_cols)]
    d2l.show_images(Y, num_rows, num_cols, scale=scale)
    
#方法举例
#图片左右翻转
apply(img, torchvision.transforms.RandomHorizontalFlip())
#图片上下翻转
apply(img, torchvision.transforms.RandomVerticalFlip())
#图片随机裁剪
shape_aug = torchvision.transforms.RandomResizedCrop(
    (200, 200), scale=(0.1, 1), ratio=(0.5, 2))
apply(img, shape_aug)

#图片修改亮度，brightness
apply(img, torchvision.transforms.ColorJitter(
    brightness=0.5, contrast=0, saturation=0, hue=0))
    
#图片修改色调，hue
apply(img, torchvision.transforms.ColorJitter(
    brightness=0, contrast=0, saturation=0, hue=0.5))
#contrast 对比度。saturation 饱和度。
```
# 微调
## 理论
一般神经网络（以分类任务为例）可以分为1：特征提取和2：线性分类器

微调指使用在源数据集（大型）上预训练的神经网络模型的特征提取层，拼接新的线性分类器，在新的数据集上训练，输出层（线性分类器）随机初始化开始训练，特征提取层微调

通常速度更快，精度更好

#### 微调训练
目标数据集上的正常训练任务，但使用更强的正则化（更小的学习率更少的数据迭代）

微调数据集：源数据集通常要大于目标数据集，能达到更好的效果

##### 重用分类器权重

源数据集中可能也有目标数据中的部分标号，可以使用预训练模型分类器中对应的向量来做初始化

##### 固定某些层
神经网络通常学习有层次的特征表示，低层次的特征更加通用（前面），高层次的特征更跟数据集有关--》可以固定底部一些层（前面几层）的参数，不参与更新

```
#微调例子
import matplotlib
from sklearn.utils import shuffle
import torch
%matplotlib inline
import os
import torchvision
from torch import nn
from d2l import torch as d2l
#以热狗数据集为例
#@save
d2l.DATA_HUB['hotdog'] = (d2l.DATA_URL + 'hotdog.zip',
                         'fba480ffa8aa7e0febbb511d181409f899b9baa5')
data_dir = d2l.download_extract('hotdog')
train_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'))
test_imgs = torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'))
#数据增强
#归一化
normalize=torchvision.transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])
#训练数据处理，随机裁剪224x224大小的图片
train_augs=torchvision.transforms.Compose([
    #裁剪
    torchvision.transforms.RandomResizedCrop(224),
    #左右翻转
    torchvision.transforms.RandomHorizontalFlip(),
    #转为张量
    torchvision.transforms.ToTensor(),
    normalize])
#测试集将图像缩放到256x256,然后裁剪中央224x224
test_augs = torchvision.transforms.Compose([
    torchvision.transforms.Resize(256),
    torchvision.transforms.CenterCrop(224),
    torchvision.transforms.ToTensor(),
    normalize])
#下载预训练模型
finetune_net = torchvision.models.resnet18(pretrained=True)
#更改全连接层结构，分类类别设为为两类
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2)
#初始化新分类器的参数
nn.init.xavier_uniform_(finetune_net.fc.weightn)
#如果param_group=True，输出层中的模型参数将使用十倍的学习率
def train_fine_tuning(net, learning_rate, batch_size=128, num_epochs=5, param_group=True):
    train_iter=torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(os.path.join(data_dir, 'train'), transform=train_augs),
                                            batch_size=batch_size,
                                            shuffle=True)
    test_iter = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'), transform=test_augs),
                                            batch_size=batch_size)
    devices = d2l.try_all_gpus()
    #reduction = 'elementwise_mean'/默认时,返回N个样本的loss求平均之后的返回
    #reduction = 'sum'时，指对n和样本的loss求和
    #reduction = 'None'时，指直接返回n分样本的loss
    loss=nn.CrossEntropyLoss(reduction='None')
    if param_group:
        #对特征提取层和输出层的模型使用不同的学习率，通过SGD实现
        params_1x = [param for name, param in net.named_parameters() if name not in ["fc.weight", "fc.bias"] ]
        trainer = torch.optim.SGD([{'params':params_1x},
                                    {'params':net.fc.parameters(), 'lr':learning_rate*10}],
                                lr=learning_rate,
                                weight_decay=0.001)
    else:
        #对所有层参数使用统一的学习率训练
        trainer=torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.001)
    d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)
if __name__ == "__main__":
    train_fine_tuning(finetune_net, 5e-5)


···